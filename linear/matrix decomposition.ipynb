{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1da05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3defc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "listening_counts = pd.read_csv('listening-counts.tsv', sep='\\t')\n",
    "users = pd.read_csv('users.tsv', sep='\\t')\n",
    "users_sample = users.sample(2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acedb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/3vj27vmj1_g4hyzs0s9k_3cm0000gn/T/ipykernel_5294/3743073923.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  counts_sample['rating'] = counts_sample['count'] / counts_sample.groupby('user_id')['count'].transform('max')\n",
      "/var/folders/6f/3vj27vmj1_g4hyzs0s9k_3cm0000gn/T/ipykernel_5294/3743073923.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_df['tracks_count'] = ratings_df.groupby('user_id')['count'].transform('count')\n"
     ]
    }
   ],
   "source": [
    "counts_sample = listening_counts[listening_counts['user_id'].isin(users_sample['user_id'])]\n",
    "counts_sample['rating'] = counts_sample['count'] / counts_sample.groupby('user_id')['count'].transform('max')\n",
    "ratings_df = counts_sample[counts_sample['track_id'].isin(\\\n",
    "            list(counts_sample.groupby('track_id').sum()['rating'].sort_values(ascending=False)[:3000].index))]\n",
    "ratings_df['tracks_count'] = ratings_df.groupby('user_id')['count'].transform('count')\n",
    "ratings_df = ratings_df[ratings_df['tracks_count'] >= 5]\n",
    "ratings_df.drop(columns=['count', 'tracks_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f839c077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# interactions on Train set: 334009\n",
      "# interactions on Test set: 83503\n"
     ]
    }
   ],
   "source": [
    "ratings_df_train, ratings_df_test = train_test_split(ratings_df,\n",
    "                                   stratify=ratings_df['user_id'], \n",
    "                                   test_size=0.20,\n",
    "                                   random_state=42)\n",
    "\n",
    "print('# interactions on Train set: %d' % len(ratings_df_train))\n",
    "print('# interactions on Test set: %d' % len(ratings_df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f2642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.set_index('user_id')\n",
    "ratings_df_train = ratings_df_train.set_index('user_id')\n",
    "ratings_df_test = ratings_df_test.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5b239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_listened(person_id, listened_df):\n",
    "    # Get the user's data and merge in the movie information.\n",
    "    listened_items = listened_df.loc[person_id]['track_id']\n",
    "    return set(listened_items if type(listened_items) == pd.Series else [listened_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fbd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = set(ratings_df['track_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5b00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "\n",
    "    def get_not_listened_items_sample(self, person_id, sample_size, seed=42):\n",
    "        listened_items = get_items_listened(person_id, ratings_df)\n",
    "        non_listened_items = all_items - listened_items\n",
    "\n",
    "        random.seed(seed)\n",
    "        non_listened_items_sample = random.sample(non_listened_items, sample_size)\n",
    "        return set(non_listened_items_sample)\n",
    "\n",
    "    def _verify_hit_top_n(self, item_id, recommended_items, topn):\n",
    "        try:\n",
    "            index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n",
    "        except:\n",
    "            index = -1\n",
    "        hit = int(index in range(0, topn))\n",
    "        return hit, index\n",
    "\n",
    "    def evaluate_model_for_user(self, model, person_id):\n",
    "        # Getting the items in test set\n",
    "        listened_values_testset = ratings_df_test.loc[person_id]\n",
    "        if type(listened_values_testset['track_id']) == pd.Series:\n",
    "            person_listened_items_testset = set(listened_values_testset['track_id'])\n",
    "        else:\n",
    "            person_listened_items_testset = set([int(listened_values_testset['track_id'])])\n",
    "        listened_items_count_testset = len(person_listened_items_testset)\n",
    "\n",
    "        # Getting a ranked recommendation list from a model for a given user\n",
    "        person_recs_df = model.recommend_items(person_id,\n",
    "                                               items_to_ignore=get_items_listened(person_id,\n",
    "                                                                                    ratings_df_train),\n",
    "                                               topn=10000000000)\n",
    "\n",
    "        hits_at_5_count = 0\n",
    "        hits_at_10_count = 0\n",
    "        # For each item the user has listened in test set\n",
    "        for item_id in person_listened_items_testset:\n",
    "            # Getting a random sample (100) items the user has not listened \n",
    "            # (to represent items that are assumed to be no relevant to the user)\n",
    "            non_listened_items_sample = self.get_not_listened_items_sample(person_id,\n",
    "                                                                               sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS,\n",
    "                                                                               seed=item_id % (2 ** 32))\n",
    "\n",
    "            # Combining the current listened item with the 100 random items\n",
    "            items_to_filter_recs = non_listened_items_sample.union(set([item_id]))\n",
    "\n",
    "            # Filtering only recommendations that are either the listened item or from a random sample of 100 non-listened items\n",
    "            valid_recs_df = person_recs_df[person_recs_df['track_id'].isin(items_to_filter_recs)]\n",
    "            valid_recs = valid_recs_df['track_id'].values\n",
    "            # Verifying if the current listened item is among the Top-N recommended items\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
    "            hits_at_5_count += hit_at_5\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
    "            hits_at_10_count += hit_at_10\n",
    "\n",
    "        # Recall is the rate of the listened items that are ranked among the Top-N recommended items, \n",
    "        # when mixed with a set of non-relevant items\n",
    "        recall_at_5 = hits_at_5_count / float(listened_items_count_testset)\n",
    "        recall_at_10 = hits_at_10_count / float(listened_items_count_testset)\n",
    "\n",
    "        person_metrics = {'hits@5_count': hits_at_5_count,\n",
    "                          'hits@10_count': hits_at_10_count,\n",
    "                          'listened_count': listened_items_count_testset,\n",
    "                          'recall@5': recall_at_5,\n",
    "                          'recall@10': recall_at_10}\n",
    "        return person_metrics\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        # print('Running evaluation for users')\n",
    "        people_metrics = []\n",
    "        for idx, person_id in enumerate(tqdm(list(ratings_df_test.index.unique().values))):\n",
    "            # if idx % 100 == 0 and idx > 0:\n",
    "            #    print('%d users processed' % idx)\n",
    "            person_metrics = self.evaluate_model_for_user(model, person_id)\n",
    "            person_metrics['user_id'] = person_id\n",
    "            people_metrics.append(person_metrics)\n",
    "        print('%d users processed' % idx)\n",
    "\n",
    "        detailed_results_df = pd.DataFrame(people_metrics) \\\n",
    "            .sort_values('listened_count', ascending=False)\n",
    "\n",
    "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(\n",
    "            detailed_results_df['listened_count'].sum())\n",
    "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(\n",
    "            detailed_results_df['listened_count'].sum())\n",
    "\n",
    "        global_metrics = {'modelName': model.get_model_name(),\n",
    "                          'recall@5': global_recall_at_5,\n",
    "                          'recall@10': global_recall_at_10}\n",
    "        return global_metrics, detailed_results_df\n",
    "\n",
    "\n",
    "model_evaluator = ModelEvaluator()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c01df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFRecommender:\n",
    "    \n",
    "    MODEL_NAME = 'Collaborative Filtering'\n",
    "    \n",
    "    def __init__(self, cf_predictions_df):\n",
    "        self.cf_predictions_df = cf_predictions_df\n",
    "        \n",
    "    def get_model_name(self):\n",
    "        return self.MODEL_NAME\n",
    "        \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=10):\n",
    "        # Get and sort the user's predictions\n",
    "        sorted_user_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False) \\\n",
    "                                    .reset_index().rename(columns={user_id: 'recStrength'})\n",
    "\n",
    "        # Recommend the highest predicted rating movies that the user hasn't seen yet.\n",
    "        recommendations_df = sorted_user_predictions[~sorted_user_predictions['track_id'].isin(items_to_ignore)] \\\n",
    "                               .sort_values('recStrength', ascending = False) \\\n",
    "                               .head(topn)\n",
    "\n",
    "        return recommendations_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f67f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_train = ratings_df.pivot_table(index='user_id', columns='track_id', values='rating').fillna(0)\n",
    "csr_coll_matrix_train = csr_matrix(pivot_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe75ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_FACTORS_MF = 15\n",
    "\n",
    "U, sigma, Vt = svds(csr_coll_matrix_train, k = NUMBER_OF_FACTORS_MF)\n",
    "\n",
    "sigma = np.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3258b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_train = np.dot(np.dot(U, sigma), Vt) \n",
    "predicted_ratings_train_norm = (predicted_ratings_train - \n",
    "                                   predicted_ratings_train.min()) / (predicted_ratings_train.max()\n",
    "                                                                        - predicted_ratings_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2272f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_preds_df = pd.DataFrame(predicted_ratings_train_norm,\n",
    "                           columns = pivot_train.columns,\n",
    "                           index=list(pivot_train.index)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760e6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_recommender_model = CFRecommender(cf_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c0e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluator = ModelEvaluator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b4694b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43529c7ffa6e46538f56a218036d7a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855 users processed\n"
     ]
    }
   ],
   "source": [
    "cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "825b3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global metrics:\n",
      "{'modelName': 'Collaborative Filtering', 'recall@5': 0.3301917296384561, 'recall@10': 0.4712166029962995}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hits@5_count</th>\n",
       "      <th>hits@10_count</th>\n",
       "      <th>listened_count</th>\n",
       "      <th>recall@5</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>81</td>\n",
       "      <td>122</td>\n",
       "      <td>371</td>\n",
       "      <td>0.218329</td>\n",
       "      <td>0.328841</td>\n",
       "      <td>48315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>164</td>\n",
       "      <td>200</td>\n",
       "      <td>358</td>\n",
       "      <td>0.458101</td>\n",
       "      <td>0.558659</td>\n",
       "      <td>6110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>114</td>\n",
       "      <td>144</td>\n",
       "      <td>358</td>\n",
       "      <td>0.318436</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>24770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>122</td>\n",
       "      <td>170</td>\n",
       "      <td>353</td>\n",
       "      <td>0.345609</td>\n",
       "      <td>0.481586</td>\n",
       "      <td>3556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>231</td>\n",
       "      <td>333</td>\n",
       "      <td>0.582583</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>28467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>121</td>\n",
       "      <td>309</td>\n",
       "      <td>0.265372</td>\n",
       "      <td>0.391586</td>\n",
       "      <td>41663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>120</td>\n",
       "      <td>149</td>\n",
       "      <td>294</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.506803</td>\n",
       "      <td>47286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>115</td>\n",
       "      <td>158</td>\n",
       "      <td>293</td>\n",
       "      <td>0.392491</td>\n",
       "      <td>0.539249</td>\n",
       "      <td>8760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>175</td>\n",
       "      <td>212</td>\n",
       "      <td>292</td>\n",
       "      <td>0.599315</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>73816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>64</td>\n",
       "      <td>101</td>\n",
       "      <td>288</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.350694</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hits@5_count  hits@10_count  listened_count  recall@5  recall@10  user_id\n",
       "104            81            122             371  0.218329   0.328841    48315\n",
       "661           164            200             358  0.458101   0.558659     6110\n",
       "192           114            144             358  0.318436   0.402235    24770\n",
       "29            122            170             353  0.345609   0.481586     3556\n",
       "3             194            231             333  0.582583   0.693694    28467\n",
       "2              82            121             309  0.265372   0.391586    41663\n",
       "23            120            149             294  0.408163   0.506803    47286\n",
       "105           115            158             293  0.392491   0.539249     8760\n",
       "313           175            212             292  0.599315   0.726027    73816\n",
       "34             64            101             288  0.222222   0.350694     1592"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nGlobal metrics:\\n%s' % cf_global_metrics)\n",
    "cf_detailed_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ab657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
